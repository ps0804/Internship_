{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "536a8de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Importing selenium webdriver \n",
    "from selenium import webdriver\n",
    "\n",
    "# Importing required Exceptions which needs to handled\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "\n",
    "#Importing requests\n",
    "import requests\n",
    "\n",
    "# importing regex\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1158d846",
   "metadata": {},
   "source": [
    "1. Scrape the details of most viewed videos on YouTube from Wikipedia:\n",
    "Url = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos/ You need to find following details:\n",
    "\n",
    "A) Rank\n",
    "B) Name\n",
    "C) Artist\n",
    "D) Upload date\n",
    "E) Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d8bc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(4)\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"\n",
    "driver.get(url)\n",
    "time.sleep(6)\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e63d14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating blank list\n",
    "Rank=[]\n",
    "Name=[]\n",
    "Artist=[]\n",
    "Upload_date=[]\n",
    "Views=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f068cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping Rank\n",
    "rank=driver.find_elements_by_xpath(\"//table[@class='wikitable sortable jquery-tablesorter'][1]/tbody/tr/td[1]\")\n",
    "for i in rank:\n",
    "    Rank.append(i.text)\n",
    "\n",
    "#Scraping Name\n",
    "name=driver.find_elements_by_xpath(\"//table[@class='wikitable sortable jquery-tablesorter'][1]/tbody/tr/td[2]\")\n",
    "for i in name:\n",
    "    Name.append(i.text)\n",
    "    \n",
    "#Scraping Artist\n",
    "artist=driver.find_elements_by_xpath(\"//table[@class='wikitable sortable jquery-tablesorter'][1]/tbody/tr/td[3]\")\n",
    "for i in artist:\n",
    "    Artist.append(i.text)\n",
    "    \n",
    "    \n",
    "#Scraping Views\n",
    "views=driver.find_elements_by_xpath(\"//table[@class='wikitable sortable jquery-tablesorter'][1]/tbody/tr/td[4]\")\n",
    "for i in views:\n",
    "    Views.append(i.text)\n",
    "    \n",
    "    \n",
    "#Scraping Upload_date\n",
    "upload_date=driver.find_elements_by_xpath(\"//table[@class='wikitable sortable jquery-tablesorter'][1]/tbody/tr/td[5]\")\n",
    "for i in upload_date:\n",
    "    Upload_date.append(i.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50e8926",
   "metadata": {},
   "outputs": [],
   "source": [
    "Youtube_viewed=pd.DataFrame({})\n",
    "Youtube_viewed['Rank']=Rank\n",
    "Youtube_viewed['Name']=Name\n",
    "Youtube_viewed['Artist']=Artist\n",
    "Youtube_viewed['Upload_date']=Upload_date\n",
    "Youtube_viewed['Views']=Views\n",
    "Youtube_viewed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7258b243",
   "metadata": {},
   "source": [
    "2. Scrape the details team Indiaâ€™s international fixtures from bcci.tv.\n",
    "Url = https://www.bcci.tv/. You need to find following details:\n",
    "        \n",
    "A) Match title (I.e. 1st ODI)\n",
    "B) Series\n",
    "C) Place\n",
    "D) Date\n",
    "E) Time\n",
    "Note: - From bcci.tv home page you have reach to the international fixture page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051ed0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(4)\n",
    "\n",
    "url = \"https://www.bcci.tv/\"\n",
    "driver.get(url)\n",
    "time.sleep(4)\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f538b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#click on international\n",
    "international=driver.find_element_by_xpath(\"/html/body/div[3]/div/div[2]/div[2]/nav/ul/li[1]/div[2]\")\n",
    "international.click()\n",
    "time.sleep(3)\n",
    "\n",
    "# click on fixtures\n",
    "fixtures= driver.find_element_by_xpath(\"/html/body/div[3]/div/div[2]/div[2]/nav/ul/li[1]/div[2]/div/ul/li[1]/a\")\n",
    "fixtures.click()\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc8cb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Empty list\n",
    "Match_title=[]\n",
    "Series=[]\n",
    "Place=[]\n",
    "Date=[]\n",
    "Time=[]\n",
    "\n",
    "#Scraping Match_title\n",
    "match_title= driver.find_elements_by_xpath(\"//strong[@class='fixture__name fixture__name--with-margin']\")\n",
    "for i in match_title:\n",
    "    Match_title.append(i.text)\n",
    "    \n",
    "#Scraping Series\n",
    "series= driver.find_elements_by_xpath(\"//span[@class='u-unskewed-text fixture__tournament-label u-truncated']\")\n",
    "for i in series:\n",
    "    Series.append(i.text)\n",
    "    \n",
    "#Scraping Place\n",
    "place= driver.find_elements_by_xpath(\"//p[@class='fixture__additional-info']/span\")\n",
    "for i in place:\n",
    "    Place.append(i.text)   \n",
    "    \n",
    "#Scraping Date\n",
    "date= driver.find_elements_by_xpath(\"//div[@class='fixture__full-date']\")\n",
    "for i in date:\n",
    "    Date.append(i.text)   \n",
    "    \n",
    "#Scraping Time\n",
    "time= driver.find_elements_by_xpath(\"//span[@class='fixture__time']\")\n",
    "for i in time:\n",
    "    Time.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd54355c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fixture=pd.DataFrame({})\n",
    "Fixture['Match_title']=Match_title\n",
    "Fixture['Series']=Series\n",
    "Fixture['Place']=Place\n",
    "Fixture['Date']=Date\n",
    "Fixture['Time']=Time\n",
    "Fixture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb0520e",
   "metadata": {},
   "source": [
    "3. Scrape the details of State-wise GDP ofIndia fromstatisticstime.com. \n",
    "Url = http://statisticstimes.com/\n",
    "You have to find following details:\n",
    "    \n",
    "A) Rank\n",
    "B) State\n",
    "C) GSDP(18-19)- at current prices\n",
    "D) GSDP(19-20)- at current prices\n",
    "E) Share(18-19)\n",
    "F) GDP($ billion)\n",
    "Note: - From statisticstimes home page you have to reach to economy page through code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed51808",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(4)\n",
    "\n",
    "url = \"http://statisticstimes.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(4)\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03e8759",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Click Economy\n",
    "Economy=driver.find_element_by_xpath(\"//*[@id='top']/div[2]/div[2]/button\")\n",
    "Economy.click()\n",
    "\n",
    "#Click India\n",
    "India= driver.find_element_by_xpath(\"//*[@id='top']/div[2]/div[2]/div/a[3]\")\n",
    "India.click()\n",
    "\n",
    "#Click on India GPD\n",
    "GDP=driver.find_element_by_xpath(\"/html/body/div[2]/div[2]/div[2]/ul/li[1]/a\")\n",
    "GDP.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec9e93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty list\n",
    "Rank=[]\n",
    "State=[]\n",
    "GSDP_at_current_price_19_20=[]\n",
    "GSDP_at_current_price_18_19=[]\n",
    "Share_18_19=[]\n",
    "GDP_billion=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7d148b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping Rank\n",
    "rank= driver.find_elements_by_xpath(\"//div[@id='table_id_wrapper']/table/tbody/tr/td[1]\")\n",
    "for i in rank:\n",
    "    if i.text is None:\n",
    "        Rank.append('--')\n",
    "    else:\n",
    "        Rank.append(i.text)\n",
    "        \n",
    "#Scraping State\n",
    "state= driver.find_elements_by_xpath(\"//div[@id='table_id_wrapper']/table/tbody/tr/td[2]\")\n",
    "for i in state:\n",
    "    if i.text is None:\n",
    "        State.append('--')\n",
    "    else:\n",
    "        State.append(i.text)\n",
    "                \n",
    "#Scraping GSDP_at_current_price_19_20\n",
    "gSDP_at_current_price_19_20= driver.find_elements_by_xpath(\"//div[@id='table_id_wrapper']/table/tbody/tr/td[3]\")\n",
    "for i in gSDP_at_current_price_19_20:\n",
    "    if i.text is None:\n",
    "        GSDP_at_current_price_19_20.append('--')\n",
    "    else:\n",
    "        GSDP_at_current_price_19_20.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb21203",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping GSDP_at_current_price_18_19\n",
    "gSDP_at_current_price_18_19= driver.find_elements_by_xpath(\"//div[@id='table_id_wrapper']/table/tbody/tr/td[4]\")\n",
    "for i in gSDP_at_current_price_18_19:\n",
    "    if i.text is None:\n",
    "        GSDP_at_current_price_18_19.append('--')\n",
    "    else:\n",
    "        GSDP_at_current_price_18_19.append(i.text)\n",
    "        \n",
    "#Scraping Share_18_19\n",
    "share_18_19= driver.find_elements_by_xpath(\"//div[@id='table_id_wrapper']/table/tbody/tr/td[5]\")\n",
    "for i in share_18_19:\n",
    "    if i.text is None:\n",
    "        Share_18_19.append('--')\n",
    "    else:\n",
    "        Share_18_19.append(i.text)\n",
    "        \n",
    "#Scraping GDP_billion\n",
    "gDP_billion= driver.find_elements_by_xpath(\"//div[@id='table_id_wrapper']/table/tbody/tr/td[6]\")\n",
    "for i in gDP_billion:\n",
    "    if i.text is None:\n",
    "        GDP_billion.append('--')\n",
    "    else:\n",
    "        GDP_billion.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e1907d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataframe\n",
    "GDP_India=pd.DataFrame({})\n",
    "GDP_India['Rank']=Rank\n",
    "GDP_India['State']=State\n",
    "GDP_India['GSDP_at_current_price_19_20']=GSDP_at_current_price_19_20\n",
    "GDP_India['GSDP_at_current_price_18_19']=GSDP_at_current_price_18_19\n",
    "GDP_India['Share_18_19']=Share_18_19\n",
    "GDP_India['GDP_billion']=GDP_billion\n",
    "GDP_India"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fde0ad1",
   "metadata": {},
   "source": [
    "4. Scrape the details of trending repositories on Github.com.\n",
    "Url = https://github.com/ You have to find the following details:\n",
    "        \n",
    "        \n",
    "A) Repository title\n",
    "B) Repository description\n",
    "C) Contributors count\n",
    "D) Language used\n",
    "Note: - From the home page you have to click on the trending option from Explore menu through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87154d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(4)\n",
    "\n",
    "url = \"https://github.com/ \"\n",
    "driver.get(url)\n",
    "time.sleep(4)\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840465a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Explore=driver.find_element_by_xpath(\"/html/body/div[1]/header/div/div[2]/nav/ul/li[4]/details/summary\")\n",
    "Explore.click()\n",
    "\n",
    "Treading=driver.find_element_by_xpath(\"/html/body/div[1]/header/div/div[2]/nav/ul/li[4]/details/div/ul[2]/li[3]/a\")\n",
    "Treading.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa0895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating emplty list \n",
    "Repository_title=[]\n",
    "Repository_description=[]\n",
    "Contributors_count=[]\n",
    "Language_used=[]\n",
    "\n",
    "#scraping Repository_title\n",
    "repository_title=driver.find_elements_by_xpath(\"//h1[@class='h3 lh-condensed']\")\n",
    "for i in repository_title:\n",
    "    if i.text is None:\n",
    "        Repository_title.append('--')\n",
    "    else:\n",
    "        Repository_title.append(i.text)\n",
    "        \n",
    "#scraping Repository_description\n",
    "repository_description=driver.find_elements_by_xpath(\"//p[@class='col-9 color-text-secondary my-1 pr-4']\")\n",
    "for i in repository_description:\n",
    "    if i.text is None:\n",
    "        Repository_description.append('--')\n",
    "    else:\n",
    "        Repository_description.append(i.text)\n",
    "        \n",
    "\n",
    "#scraping Contributors_count\n",
    "contributors_count=driver.find_elements_by_xpath(\"//div[@class='f6 color-text-secondary mt-2']/a[2]\")\n",
    "for i in contributors_count:\n",
    "    if i.text is None:\n",
    "        Contributors_count.append('--')\n",
    "    else:\n",
    "        Contributors_count.append(i.text)\n",
    "        \n",
    "\n",
    "#scraping Language_used\n",
    "language_used=driver.find_elements_by_xpath(\"//div[@class='f6 color-text-secondary mt-2']/span[1]\")\n",
    "for i in language_used:\n",
    "    if i.text is None:\n",
    "        Language_used.append('--')\n",
    "    else:\n",
    "        Language_used.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c513ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Treding_rep=pd.DataFrame({})\n",
    "Treding_rep['Repository_title']= Repository_title\n",
    "Treding_rep['Repository_description']= Repository_description\n",
    "Treding_rep['Contributors_count']= Contributors_count\n",
    "Treding_rep['Language_used']= Language_used\n",
    "Treding_rep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc244369",
   "metadata": {},
   "source": [
    "5. Scrape the details of top 100 songs on billboard.com.\n",
    "Url = https://www.billboard.com/\n",
    "You have to find the following details:\n",
    "    \n",
    "A) Song name\n",
    "B) Artist name\n",
    "C) Last week rank\n",
    "D) Peak rank\n",
    "E) Weeks on board\n",
    "Note: - From the home page you have to click on the charts option then hot 100-page link through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785f3b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(4)\n",
    "\n",
    "url = \"https://www.billboard.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(6)\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d076dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(3)\n",
    "Hot_song= driver.find_element_by_xpath(\"//li[@class='header__subnav__item'][3]\")\n",
    "Hot_song.click()\n",
    "time.sleep(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48453c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Blank List\n",
    "Song_name=[]\n",
    "Artist_name=[]\n",
    "Last_week_rank=[]\n",
    "Peak_rank=[]\n",
    "Weeks_on_board=[]\n",
    "\n",
    "#Scraping Song_name\n",
    "song_name= driver.find_elements_by_xpath(\"//span[@class='chart-element__information']/span[1]\")\n",
    "for i in song_name:\n",
    "    if i.text is None:\n",
    "        Song_name.append('--')\n",
    "    else:\n",
    "        Song_name.append(i.text)\n",
    "\n",
    "#Scraping Artist_name\n",
    "artist_name= driver.find_elements_by_xpath(\"//span[@class='chart-element__information']/span[2]\")\n",
    "for i in artist_name:\n",
    "    if i.text is None:\n",
    "        Artist_name.append('--')\n",
    "    else:\n",
    "        Artist_name.append(i.text)\n",
    "        \n",
    "#Scraping Last_week_rank\n",
    "last_week_rank= driver.find_elements_by_xpath(\"//div[@class='chart-element__meta text--center color--secondary text--last']\")\n",
    "for i in last_week_rank:\n",
    "    if i.text is None:\n",
    "        Last_week_rank.append('--')\n",
    "    else:\n",
    "        Last_week_rank.append(i.text)\n",
    "        \n",
    "#Scraping Peak_rank\n",
    "peak_rank= driver.find_elements_by_xpath(\"//div[@class='chart-element__meta text--center color--secondary text--peak']\")\n",
    "for i in peak_rank:\n",
    "    if i.text is None:\n",
    "        Peak_rank.append('--')\n",
    "    else:\n",
    "        Peak_rank.append(i.text)\n",
    "        \n",
    "#Scraping Weeks_on_board\n",
    "weeks_on_board= driver.find_elements_by_xpath(\"//div[@class='chart-element__meta text--center color--secondary text--week']\")\n",
    "for i in weeks_on_board:\n",
    "    if i.text is None:\n",
    "        Weeks_on_board.append('--')\n",
    "    else:\n",
    "        Weeks_on_board.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3da682",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hot_Songs=pd.DataFrame({})\n",
    "Hot_Songs['Song_name']=Song_name\n",
    "Hot_Songs['Artist_name']=Artist_name\n",
    "Hot_Songs['Last_week_rank']=Last_week_rank\n",
    "Hot_Songs['Peak_rank']=Peak_rank\n",
    "Hot_Songs['Weeks_on_board']=Weeks_on_board\n",
    "\n",
    "Hot_Songs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6e41fa",
   "metadata": {},
   "source": [
    "6. Scrape the details of Highest selling novels.\n",
    "Url = https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare/ You have to find the following details:\n",
    "        \n",
    "        \n",
    "A) Book name\n",
    "B) Author name\n",
    "C) Volumes sold\n",
    "D) Publisher\n",
    "E) Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d95b5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9e84b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare/\"\n",
    "driver.get(url)\n",
    "time.sleep(6)\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a622b6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty lists for scraping data\n",
    "Book_name=[]\n",
    "Author_name=[]\n",
    "Volumes_sold=[]\n",
    "Publisher=[]\n",
    "Genre=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbde83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping Book_name\n",
    "book_name=driver.find_elements_by_xpath(\"//table[@class='in-article sortable']/tbody/tr/td[2]\")\n",
    "for i in book_name:\n",
    "    if i.text is None:\n",
    "        Book_name.append('--')\n",
    "    else:\n",
    "        Book_name.append(i.text)\n",
    "        \n",
    "# Scraping Author_name\n",
    "author_name=driver.find_elements_by_xpath(\"//table[@class='in-article sortable']/tbody/tr/td[3]\")\n",
    "for i in author_name:\n",
    "    if i.text is None:\n",
    "        Author_name.append('--')\n",
    "    else:\n",
    "        Author_name.append(i.text)\n",
    "        \n",
    "# Scraping Volumes_sold\n",
    "volumes_sold=driver.find_elements_by_xpath(\"//table[@class='in-article sortable']/tbody/tr/td[4]\")\n",
    "for i in volumes_sold:\n",
    "    if i.text is None:\n",
    "        Volumes_sold.append('--')\n",
    "    else:\n",
    "        Volumes_sold.append(i.text)\n",
    "        \n",
    "# Scraping Author_name\n",
    "publisher=driver.find_elements_by_xpath(\"//table[@class='in-article sortable']/tbody/tr/td[5]\")\n",
    "for i in publisher:\n",
    "    if i.text is None:\n",
    "        Publisher.append('--')\n",
    "    else:\n",
    "        Publisher.append(i.text)\n",
    "        \n",
    "# Scraping Genre\n",
    "genre=driver.find_elements_by_xpath(\"//table[@class='in-article sortable']/tbody/tr/td[6]\")\n",
    "for i in genre:\n",
    "    if i.text is None:\n",
    "        Genre.append('--')\n",
    "    else:\n",
    "        Genre.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94e51b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Top_selling_book=pd.DataFrame({})\n",
    "Top_selling_book['Book_name']=Book_name\n",
    "Top_selling_book['Author_name']=Author_name\n",
    "Top_selling_book['Volumes_sold']=Volumes_sold\n",
    "Top_selling_book['Publisher']=Publisher\n",
    "Top_selling_book['Genre']=Genre\n",
    "Top_selling_book"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37204c8",
   "metadata": {},
   "source": [
    "7. Scrape the details most watched tv series of all time from imdb.com.\n",
    "Url = https://www.imdb.com/list/ls095964455/ You have to find the following details: \n",
    "        \n",
    "A) Name\n",
    "B) Year span\n",
    "C) Genre\n",
    "D) Run time\n",
    "E) Ratings\n",
    "F) Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8e3a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "url =\"https://www.imdb.com/list/ls095964455/\"\n",
    "driver.get(url)\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e544e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating empty list\n",
    "Name=[]\n",
    "Year_span=[]\n",
    "Genre=[]\n",
    "Run_time=[]\n",
    "Ratings=[]\n",
    "Votes=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ebed06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping Name\n",
    "name=driver.find_elements_by_xpath(\"//h3[@class='lister-item-header']/a\")\n",
    "for i in name:\n",
    "    Name.append(i.text)\n",
    "    \n",
    "#Scraping Year_span\n",
    "year_span=driver.find_elements_by_xpath(\"//h3[@class='lister-item-header']/span[2]\")\n",
    "for i in year_span:\n",
    "    Year_span.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping Genre\n",
    "genre=driver.find_elements_by_xpath(\"//p[@class='text-muted text-small']/span[5]\")\n",
    "for i in genre:\n",
    "    Genre.append(i.text)\n",
    "    \n",
    "    \n",
    "#Scraping Run_time\n",
    "run_time=driver.find_elements_by_xpath(\"//p[@class='text-muted text-small']/span[3]\")\n",
    "for i in run_time:\n",
    "    Run_time.append(i.text)\n",
    "    \n",
    "#Scraping Ratings\n",
    "ratings=driver.find_elements_by_xpath(\"//div[@class='ipl-rating-widget']/div/span[2]\")\n",
    "for i in ratings:\n",
    "    Ratings.append(i.text)\n",
    "    \n",
    "#Scraping Votes\n",
    "votes=driver.find_elements_by_xpath(\"//p[@class='text-muted text-small'][3]/span[2]\")\n",
    "for i in votes:\n",
    "    Votes.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829f97a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Top_shows=pd.DataFrame({})\n",
    "Top_shows['Name']=Name\n",
    "Top_shows['Year_span']=Year_span\n",
    "Top_shows['Genre']=Genre\n",
    "Top_shows['Run_time']=Run_time\n",
    "Top_shows['Ratings']=Ratings\n",
    "Top_shows['Votes']=Votes\n",
    "Top_shows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ccfc49",
   "metadata": {},
   "source": [
    "8. Details of Datasets from UCI machine learning repositories.\n",
    "Url = https://archive.ics.uci.edu/\n",
    "You have to find the following details:\n",
    "    \n",
    "    \n",
    "A) Dataset name\n",
    "B) Data type\n",
    "C) Task\n",
    "D) Attribute type\n",
    "E) No of instances\n",
    "F) No of attribute\n",
    "G) Year\n",
    "Note: - from the home page you have to go to the Show All Dataset page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a53ee9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(4)\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/\"\n",
    "driver.get(url)\n",
    "time.sleep(6)\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e88696",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_dataste= driver.find_element_by_xpath(\"/html/body/table[1]/tbody/tr/td[2]/span[2]/a\")\n",
    "view_dataste.click()\n",
    "time.sleep(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03c351d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty lists for scraping data\n",
    "Dataset_name=[]\n",
    "Data_type=[]\n",
    "Task=[]\n",
    "Attribute_type=[]\n",
    "No_of_instances=[]\n",
    "No_of_attribute=[]\n",
    "Year=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf69ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping Dataset name\n",
    "data_name= driver.find_elements_by_xpath(\"//p[@class='normal']/b/a[1]\")\n",
    "for i in data_name:\n",
    "    if i.text is None :\n",
    "        Dataset_name.append(\"--\") \n",
    "    else:\n",
    "        Dataset_name.append(i.text)\n",
    "\n",
    "time.sleep(4)        \n",
    "#Scraping data Type\n",
    "data_type=driver.find_elements_by_xpath(\"//table[@border='1']/tbody/tr/td[2]/p\")\n",
    "for i in data_type[1:]:\n",
    "    if i.text is None :\n",
    "        Data_type.append(\"--\") \n",
    "    else:\n",
    "        Data_type.append(i.text)\n",
    "\n",
    "time.sleep(4) \n",
    "#Scraping Task\n",
    "task=driver.find_elements_by_xpath(\"//table[@border='1']/tbody/tr/td[3]/p\")\n",
    "for i in task[1:]:\n",
    "    if i.text is None :\n",
    "        Task.append(\"--\") \n",
    "    else:\n",
    "        Task.append(i.text)\n",
    "        \n",
    "time.sleep(4) \n",
    "#Scraping Attribute_type\n",
    "attribute_type=driver.find_elements_by_xpath(\"//table[@border='1']/tbody/tr/td[4]/p\")\n",
    "for i in attribute_type[1:]:\n",
    "    if i.text is None :\n",
    "        Attribute_type.append(\"--\") \n",
    "    else:\n",
    "        Attribute_type.append(i.text)\n",
    "        time.sleep(4)      \n",
    "        \n",
    "#Scraping No_of_instances\n",
    "no_of_instances=driver.find_elements_by_xpath(\"//table[@border='1']/tbody/tr/td[5]/p\")\n",
    "for i in no_of_instances[1:]:\n",
    "    if i.text is None :\n",
    "        No_of_instances.append(\"--\") \n",
    "    else:\n",
    "        No_of_instances.append(i.text)\n",
    "\n",
    "time.sleep(4)         \n",
    "#Scraping No_of_attribute\n",
    "no_of_attribute=driver.find_elements_by_xpath(\"//table[@border='1']/tbody/tr/td[6]/p\")\n",
    "for i in no_of_attribute[1:]:\n",
    "    if i.text is None :\n",
    "        No_of_attribute.append(\"--\") \n",
    "    else:\n",
    "        No_of_attribute.append(i.text)\n",
    "\n",
    "time.sleep(4) \n",
    "#Scraping Year\n",
    "year=driver.find_elements_by_xpath(\"//table[@border='1']/tbody/tr/td[7]/p\")\n",
    "for i in year[1:]:\n",
    "    if i.text is None :\n",
    "        Year.append(\"--\") \n",
    "    else:\n",
    "        Year.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9452a2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dataset=pd.DataFrame({})\n",
    "data_dataset['Dataset_name']= Dataset_name\n",
    "data_dataset['Data_type']= Data_type\n",
    "data_dataset['Task']= Task\n",
    "data_dataset['Attribute_type']= Attribute_type\n",
    "data_dataset['No_of_instances']= No_of_instances\n",
    "data_dataset['No_of_attribute']= No_of_attribute\n",
    "data_dataset['Year']= Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c4a624",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d49ba4",
   "metadata": {},
   "source": [
    "9. Scrape the details of Data science recruiters Url = https://www.naukri.com/hr-recruiters-consultants\n",
    "You have to find the following details: \n",
    "    \n",
    "    \n",
    "A) Name\n",
    "B) Designation\n",
    "C)Company \n",
    "D)Skills they hire for \n",
    "E) Location\n",
    "Note: - From naukri.com homepage click on the recruiters option and the on the search pane type Data science and \n",
    "click on search. All this should be done through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fea31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(4)\n",
    "\n",
    "url = \"https://www.naukri.com/recruiters/\"\n",
    "driver.get(url)\n",
    "time.sleep(6)\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e273d6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "search=driver.find_element_by_xpath(\"//input[@class='sugInp']\")\n",
    "search.send_keys('Data Science')\n",
    "\n",
    "search_btn=driver.find_element_by_xpath(\"//button[@class='fl qsbSrch blueBtn']\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f9f27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating emepty list\n",
    "Name=[]\n",
    "Designation=[]\n",
    "Company=[]\n",
    "Skills_they_hire=[]\n",
    "Location=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dec8e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping Name \n",
    "name=driver.find_elements_by_xpath(\"//span[@class='fl ellipsis']\")\n",
    "for i  in name:\n",
    "    Name.append(i.text)\n",
    "    \n",
    "#Scraping Designation \n",
    "designation=driver.find_elements_by_xpath(\"//span[@class='ellipsis clr']\")\n",
    "for i  in designation:\n",
    "    Designation.append(i.text)    \n",
    "    \n",
    "#Scraping Company \n",
    "company=driver.find_elements_by_xpath(\"//p[@class='highlightable']/a[2]\")\n",
    "for i  in company:\n",
    "    Company.append(i.text)   \n",
    "    \n",
    "    \n",
    "#Scraping Skills_they_hire \n",
    "skills_they_hire=driver.find_elements_by_xpath(\"//div[@class='hireSec highlightable']\")\n",
    "for i  in skills_they_hire:\n",
    "    Skills_they_hire.append(i.text)\n",
    "    \n",
    "\n",
    "    \n",
    "#Scraping Location \n",
    "location=driver.find_elements_by_xpath(\"//p[@class='highlightable']/span[2]\")\n",
    "for i  in location:\n",
    "    if i.text is None:\n",
    "        \n",
    "        Location.append('--')\n",
    "    else:\n",
    "        Location.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a2a129",
   "metadata": {},
   "outputs": [],
   "source": [
    "Recruiters=pd.DataFrame({})\n",
    "Recruiters['Name']= Name[0:49]\n",
    "Recruiters['Designation']= Designation[0:49]\n",
    "Recruiters['Company']= Company[0:49]\n",
    "Recruiters['Skills_they_hire']= Skills_they_hire[0:49]\n",
    "Recruiters['Location']= Location[0:49]\n",
    "Recruiters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d506777a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Location"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
